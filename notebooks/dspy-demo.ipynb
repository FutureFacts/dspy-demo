{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/niels/Code/dspy-demo/notebooks/docs')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "base_path = Path(\".\") / \"docs\"\n",
    "base_path.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma-db/\")\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"papers\", embedding_function=sentence_transformer_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('docs/1706.03762v7.pdf')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs = list(base_path.glob(\"*.pdf\"))\n",
    "list(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(pdfs[0])\n",
    "documents = pdf_loader.load_and_split()\n",
    "# print(len(documents))\n",
    "docs = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"papers\", embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "collection.add(documents=docs, ids=list(map(str, range(len(docs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = ChromadbRM(\n",
    "    \"papers\", \"chroma-db/\", embedding_function=sentence_transformer_ef, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\"openai/gpt-4o-mini\")\n",
    "\n",
    "dspy.settings.configure(lm=lm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> response\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, response=prediction.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    context=['[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12', 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'],\n",
       "    response='The authors of the paper \"Attention Is All You Need\" are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.'\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_rag = RAG()\n",
    "\n",
    "resp = uncompiled_rag(\"Who are the authors of the paper?\")\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: The authors of the paper \"Attention Is All You Need\" are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\n",
      "Retrieved Contexts (truncated):\n",
      "\t[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "c\n",
      "\tTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b\n",
      "\tProvided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted Answer: {resp.response}\")\n",
    "print(\"Retrieved Contexts (truncated):\")\n",
    "for c in resp.context:\n",
    "    print(f\"\\t{c[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The context discusses advancements in natural language processing (NLP) through various research papers, particularly focusing on the Transformer architecture and attention mechanisms. It highlights the effectiveness of self-training for parsing, the use of attention in handling long-distance dependencies, and variations in Transformer models that impact performance metrics like BLEU scores and perplexity. Additionally, it mentions experiments conducted on English constituency parsing using the Penn Treebank, demonstrating the model's ability to generalize across tasks and the importance of parameters such as dropout and attention dimensions in achieving state-of-the-art results.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = uncompiled_rag(\"Can you summarize it?\")\n",
    "resp.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context discusses advancements in natural language processing (NLP) through various research papers, particularly focusing on the Transformer architecture and attention mechanisms. It highlights the effectiveness of self-training for parsing, the use of attention in handling long-distance dependencies, and variations in Transformer models that impact performance metrics like BLEU scores and perplexity. Additionally, it mentions experiments conducted on English constituency parsing using the Penn Treebank, demonstrating the model's ability to generalize across tasks and the importance of parameters such as dropout and attention dimensions in achieving state-of-the-art results.\n"
     ]
    }
   ],
   "source": [
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer model is a groundbreaking architecture in the field of natural language processing and sequence modeling. Unlike traditional models that rely on recurrent neural networks (RNNs) or convolutional networks, the Transformer uses self-attention mechanisms to process input sequences. This allows it to capture relationships between words regardless of their position in the sequence, which is particularly beneficial for tasks like translation and summarization.\n",
      "\n",
      "The architecture consists of an encoder and a decoder. The encoder processes the input sequence and generates a set of attention-based representations, while the decoder uses these representations to produce the output sequence. One of the key innovations of the Transformer is the use of multi-head attention, which allows the model to focus on different parts of the input sequence simultaneously, enhancing its ability to understand context and relationships.\n",
      "\n",
      "Additionally, the Transformer architecture enables significant parallelization during training, which leads to faster training times and improved performance on large datasets. This has made it a popular choice for various applications, including language translation, text generation, and more.\n",
      "\n",
      "If you have specific aspects of the Transformer or attention mechanisms you would like to know more about, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "resp = uncompiled_rag(\"Tell me more?\")\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = [\n",
    "    {\n",
    "        \"question\": \"What is the primary innovation introduced by the Transformer model?\",\n",
    "        \"response\": \"The Transformer model introduces an architecture based solely on self-attention, eliminating recurrent and convolutional layers.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why does the Transformer allow for faster training than RNN-based models?\",\n",
    "        \"response\": \"The Transformer enables parallelization across input sequences, reducing training time significantly compared to RNNs.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does self-attention work in the Transformer model?\",\n",
    "        \"response\": \"Self-attention allows each position in a sequence to attend to other positions, capturing dependencies regardless of their distance.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the main advantage of using multi-head attention?\",\n",
    "        \"response\": \"Multi-head attention allows the model to jointly attend to information from different representation subspaces, improving learning.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What score did the Transformer achieve on the WMT 2014 English-to-German translation task?\",\n",
    "        \"response\": \"The Transformer achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is positional encoding, and why is it used in the Transformer?\",\n",
    "        \"response\": \"Positional encoding provides information about the position of tokens, as the Transformer lacks recurrent or convolutional structures.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What type of attention function is used in the Transformer?\",\n",
    "        \"response\": \"The Transformer uses scaled dot-product attention, where attention scores are scaled to prevent large values from dominating.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many layers does the base Transformer model use for its encoder and decoder?\",\n",
    "        \"response\": \"The base Transformer model uses six layers each for the encoder and decoder.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What optimization technique is applied during training of the Transformer?\",\n",
    "        \"response\": \"The Adam optimizer with a custom learning rate schedule is used during Transformer training.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the Transformer perform on the English constituency parsing task?\",\n",
    "        \"response\": \"The Transformer model achieves competitive results on English constituency parsing, outperforming some previous models.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': 'What is the primary innovation introduced by the Transformer model?', 'response': 'The Transformer model introduces an architecture based solely on self-attention, eliminating recurrent and convolutional layers.'}) (input_keys={'question'}),\n",
       " Example({'question': 'Why does the Transformer allow for faster training than RNN-based models?', 'response': 'The Transformer enables parallelization across input sequences, reducing training time significantly compared to RNNs.'}) (input_keys={'question'}),\n",
       " Example({'question': 'How does self-attention work in the Transformer model?', 'response': 'Self-attention allows each position in a sequence to attend to other positions, capturing dependencies regardless of their distance.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the main advantage of using multi-head attention?', 'response': 'Multi-head attention allows the model to jointly attend to information from different representation subspaces, improving learning.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What score did the Transformer achieve on the WMT 2014 English-to-German translation task?', 'response': 'The Transformer achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is positional encoding, and why is it used in the Transformer?', 'response': 'Positional encoding provides information about the position of tokens, as the Transformer lacks recurrent or convolutional structures.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What type of attention function is used in the Transformer?', 'response': 'The Transformer uses scaled dot-product attention, where attention scores are scaled to prevent large values from dominating.'}) (input_keys={'question'}),\n",
       " Example({'question': 'How many layers does the base Transformer model use for its encoder and decoder?', 'response': 'The base Transformer model uses six layers each for the encoder and decoder.'}) (input_keys={'question'}),\n",
       " Example({'question': 'What optimization technique is applied during training of the Transformer?', 'response': 'The Adam optimizer with a custom learning rate schedule is used during Transformer training.'}) (input_keys={'question'}),\n",
       " Example({'question': 'How does the Transformer perform on the English constituency parsing task?', 'response': 'The Transformer model achieves competitive results on English constituency parsing, outperforming some previous models.'}) (input_keys={'question'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [dspy.Example(**d).with_inputs(\"question\") for d in qa]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\t\tWhat is the primary innovation introduced by the Transformer model?\n",
      "\n",
      "Gold Reponse:\t\tThe Transformer model introduces an architecture based solely on self-attention, eliminating recurrent and convolutional layers.\n",
      "\n",
      "Predicted Response:\tThe primary innovation introduced by the Transformer model is the use of self-attention mechanisms, allowing the model to process words in a sentence in parallel and weigh their importance regardless of their position. This architecture significantly reduces training times and costs compared to traditional RNN-based models.\n",
      "\n",
      "Semantic F1 Score:\t\t0.71\n"
     ]
    }
   ],
   "source": [
    "metric = SemanticF1()\n",
    "\n",
    "example = data[0]\n",
    "\n",
    "rag = RAG()\n",
    "pred = rag(**example.inputs())\n",
    "\n",
    "score = metric(example, pred)\n",
    "\n",
    "print(f\"Question:\\t\\t{example.question}\\n\")\n",
    "print(f\"Gold Reponse:\\t\\t{example.response}\\n\")\n",
    "print(f\"Predicted Response:\\t{pred.response}\\n\")\n",
    "print(f\"Semantic F1 Score:\\t\\t{score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    context=['Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', 'Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'],\n",
       "    response='The primary innovation introduced by the Transformer model is the use of self-attention mechanisms, allowing the model to process words in a sentence in parallel and weigh their importance regardless of their position. This architecture significantly reduces training times and costs compared to traditional RNN-based models.'\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = dspy.Evaluate(\n",
    "    devset=data, metric=metric, num_threads=24, display_progress=True, display_table=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.9322370342139 / 10  (69.3): 100%|██████████| 10/10 [00:12<00:00,  1.26s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4cccc th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_4cccc td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_4cccc_row0_col0, #T_4cccc_row0_col1, #T_4cccc_row0_col2, #T_4cccc_row0_col3, #T_4cccc_row0_col4, #T_4cccc_row1_col0, #T_4cccc_row1_col1, #T_4cccc_row1_col2, #T_4cccc_row1_col3, #T_4cccc_row1_col4, #T_4cccc_row2_col0, #T_4cccc_row2_col1, #T_4cccc_row2_col2, #T_4cccc_row2_col3, #T_4cccc_row2_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4cccc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4cccc_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_4cccc_level0_col1\" class=\"col_heading level0 col1\" >example_response</th>\n",
       "      <th id=\"T_4cccc_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n",
       "      <th id=\"T_4cccc_level0_col3\" class=\"col_heading level0 col3\" >pred_response</th>\n",
       "      <th id=\"T_4cccc_level0_col4\" class=\"col_heading level0 col4\" >SemanticF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4cccc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4cccc_row0_col0\" class=\"data row0 col0\" >What is the primary innovation introduced by the Transformer model?</td>\n",
       "      <td id=\"T_4cccc_row0_col1\" class=\"data row0 col1\" >The Transformer model introduces an architecture based solely on self-attention, eliminating recurrent and convolutional layers.</td>\n",
       "      <td id=\"T_4cccc_row0_col2\" class=\"data row0 col2\" >['Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU...</td>\n",
       "      <td id=\"T_4cccc_row0_col3\" class=\"data row0 col3\" >The primary innovation introduced by the Transformer model is the use of self-attention mechanisms, allowing the model to process words in a sentence in parallel...</td>\n",
       "      <td id=\"T_4cccc_row0_col4\" class=\"data row0 col4\" >✔️ [0.708]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4cccc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4cccc_row1_col0\" class=\"data row1 col0\" >Why does the Transformer allow for faster training than RNN-based models?</td>\n",
       "      <td id=\"T_4cccc_row1_col1\" class=\"data row1 col1\" >The Transformer enables parallelization across input sequences, reducing training time significantly compared to RNNs.</td>\n",
       "      <td id=\"T_4cccc_row1_col2\" class=\"data row1 col2\" >['Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder,...</td>\n",
       "      <td id=\"T_4cccc_row1_col3\" class=\"data row1 col3\" >The Transformer allows for faster training than RNN-based models because it uses self-attention mechanisms that enable parallel processing of input sequences. Unlike RNNs, which process...</td>\n",
       "      <td id=\"T_4cccc_row1_col4\" class=\"data row1 col4\" >✔️ [1.000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4cccc_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4cccc_row2_col0\" class=\"data row2 col0\" >How does self-attention work in the Transformer model?</td>\n",
       "      <td id=\"T_4cccc_row2_col1\" class=\"data row2 col1\" >Self-attention allows each position in a sequence to attend to other positions, capturing dependencies regardless of their distance.</td>\n",
       "      <td id=\"T_4cccc_row2_col2\" class=\"data row2 col2\" >['Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder,...</td>\n",
       "      <td id=\"T_4cccc_row2_col3\" class=\"data row2 col3\" >Self-attention in the Transformer model works by allowing each position in the input sequence to attend to all other positions. It computes a weighted sum...</td>\n",
       "      <td id=\"T_4cccc_row2_col4\" class=\"data row2 col4\" >✔️ [0.797]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x145d53d70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 7 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "69.32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING WITH THE FOLLOWING MEDIUM AUTO RUN SETTINGS:\n",
      "num_trials: 25\n",
      "minibatch: False\n",
      "num_candidates: 19\n",
      "valset size: 3\n",
      "\n",
      "\n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "Bootstrapping N=19 sets of demonstrations...\n",
      "Bootstrapping set 1/19\n",
      "Bootstrapping set 2/19\n",
      "Bootstrapping set 3/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:15<00:20,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n",
      "Bootstrapping set 4/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:01<00:11,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Bootstrapping set 5/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:03<00:18,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Bootstrapping set 6/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:01<00:10,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Bootstrapping set 7/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:01<00:11,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Bootstrapping set 8/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:05<00:33,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Bootstrapping set 9/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [06:50<17:05, 205.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n",
      "Bootstrapping set 10/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:08<00:21,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n",
      "Bootstrapping set 11/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:06<00:38,  6.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Bootstrapping set 12/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:06<00:40,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Bootstrapping set 13/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:14<00:37,  7.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n",
      "Bootstrapping set 14/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:02<00:17,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "Bootstrapping set 15/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:04<00:11,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n",
      "Bootstrapping set 16/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:00<00:00, 53.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n",
      "Bootstrapping set 17/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:09<00:22,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n",
      "Bootstrapping set 18/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:19<00:14,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n",
      "Bootstrapping set 19/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:08<00:52,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "\n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "\n",
      "Proposing instructions...\n",
      "\n",
      "Proposed Instructions for Predictor 0:\n",
      "\n",
      "0: Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "1: Using the provided `context` that contains relevant information about the Transformer model and the `question` that needs to be answered, systematically analyze the context to derive a well-reasoned `response`. Ensure to think critically and step-by-step to accurately reflect the information from the context in your answer.\n",
      "\n",
      "2: Utilize the provided `context` and `question` to generate a detailed `response` that explains the relevant concepts, ensuring clarity and coherence in your reasoning process.\n",
      "\n",
      "3: Using the provided `context` and `question`, analyze the context to derive logical reasoning and generate a detailed `response` that accurately answers the question based on the information given.\n",
      "\n",
      "4: Using the provided `context` and `question`, analyze the information step by step to generate a detailed `response` that accurately addresses the question based on the context. Ensure that your reasoning is clear and structured, highlighting key aspects of the context that support your answer.\n",
      "\n",
      "5: Using the provided `context` and `question`, please generate a detailed `response` that accurately answers the question based on the information contained in the context. Ensure to include reasoning that explains how the answer was derived from the context.\n",
      "\n",
      "6: You are an expert in machine learning and natural language processing. Given the fields `context` (which contains relevant information about the Transformer model) and `question` (which is a specific inquiry regarding that context), produce the fields `reasoning` (articulating how you derive the answer) and `response` (providing a coherent answer based on the context).\n",
      "\n",
      "7: Using the provided `context` and `question`, generate a `response` that accurately answers the question while including reasoning based on the context.\n",
      "\n",
      "8: Using the provided `context` and `question`, generate a detailed `response` that accurately answers the question based on the information in the context, while also including a clear reasoning process that outlines how you arrived at the answer.\n",
      "\n",
      "9: Imagine you are a researcher who has been tasked with developing a cutting-edge AI system for a major tech company. This system will be used to answer complex questions about the Transformer model in natural language processing. Given the fields `context` and `question`, your mission is to extract the most accurate and insightful `response`. Use your understanding of the Transformer architecture, training efficiency, and attention mechanisms to provide a well-reasoned answer. Make sure to explain your reasoning step by step to ensure clarity and transparency in your response.\n",
      "\n",
      "10: Using the provided `context` about the Transformer model and the `question` regarding its training efficiency compared to RNNs, generate a detailed `response` that explains the reasons behind the Transformer's faster training capabilities. Ensure to incorporate logical reasoning in your explanation.\n",
      "\n",
      "11: Using the provided `context` and `question`, please generate a detailed `response` that articulates a step-by-step reasoning process to arrive at an informative answer.\n",
      "\n",
      "12: Using the provided `context` that contains relevant information about the Transformer model and the `question` that you need to answer, carefully analyze the context to extract key details. Then, formulate a comprehensive `response` that directly addresses the question by synthesizing the information from the context with clear reasoning. Ensure that your response is coherent and reflects a deep understanding of the concepts related to the Transformer model, particularly focusing on mechanisms like self-attention and their significance in natural language processing.\n",
      "\n",
      "13: Imagine you are an AI assistant in a critical situation where a team of researchers is relying on your ability to provide accurate and detailed answers about the Transformer model. Given the fields `context` (which contains crucial information about the model's architecture and mechanisms) and `question` (which seeks specific insights), your task is to produce a well-reasoned `response` that directly addresses the question, ensuring clarity and depth in your answer. Remember, the researchers' progress depends on your response, so think carefully and articulate your reasoning step by step to ensure a comprehensive understanding.\n",
      "\n",
      "14: Using the provided `context`, which contains relevant information, and the `question` that seeks specific insights, generate a detailed and coherent `response`. Ensure that your response articulates a clear reasoning process that explains how the information from the context relates to the question, highlighting key points and concepts effectively.\n",
      "\n",
      "15: Using the provided `context` and `question`, generate a detailed `response` that includes the answer to the question along with a clear and logical `reasoning` process that explains how the answer was derived from the context. Ensure that the response is coherent and directly addresses the question while providing sufficient context for understanding.\n",
      "\n",
      "16: Using the provided `context` and `question`, generate a detailed `response` that explains the key concepts related to the Transformer model, ensuring that the reasoning process is clear and step-by-step.\n",
      "\n",
      "17: You are an expert in natural language processing and machine learning. Given the fields `context` and `question`, analyze the information and produce a coherent `response` that accurately answers the question based on the provided context.\n",
      "\n",
      "18: Using the provided `context` and `question`, generate a detailed response that explains the concept in question by reasoning through the context step by step. Ensure that the response is coherent, contextually relevant, and demonstrates a clear understanding of the topic at hand.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating the default program...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.7077464788732395 / 3  (56.9): 100%|██████████| 3/3 [00:00<00:00, 56.62it/s]\n",
      "/Users/niels/Code/dspy-demo/.venv/lib/python3.12/site-packages/optuna/_experimental.py:30: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default program score: 56.92\n",
      "\n",
      "==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "===== Trial 1 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mBest full score so far!\u001b[0m Score: 59.9\n",
      "Score: 59.9 with parameters ['Predictor 1: Instruction 12', 'Predictor 1: Few-Shot Set 7'].\n",
      "Scores so far: [56.92, 59.9]\n",
      "Best score so far: 59.9\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 2 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.774193548387097 / 3  (59.1): 100%|██████████| 3/3 [00:05<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.14 with parameters ['Predictor 1: Instruction 10', 'Predictor 1: Few-Shot Set 7'].\n",
      "Scores so far: [56.92, 59.9, 59.14]\n",
      "Best score so far: 59.9\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 3 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.5 / 3  (50.0): 100%|██████████| 3/3 [00:06<00:00,  2.04s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 50.0 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 18'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0]\n",
      "Best score so far: 59.9\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 4 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.7077464788732395 / 3  (56.9): 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 56.92 with parameters ['Predictor 1: Instruction 15', 'Predictor 1: Few-Shot Set 2'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92]\n",
      "Best score so far: 59.9\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 5 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 8', 'Predictor 1: Few-Shot Set 18'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9]\n",
      "Best score so far: 59.9\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 6 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.9229940764674205 / 3  (64.1): 100%|██████████| 3/3 [00:08<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mBest full score so far!\u001b[0m Score: 64.1\n",
      "Score: 64.1 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 1'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1]\n",
      "Best score so far: 64.1\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 7 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:05<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 12'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9]\n",
      "Best score so far: 64.1\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 8 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 11', 'Predictor 1: Few-Shot Set 13'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9]\n",
      "Best score so far: 64.1\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 9 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.857 / 3  (61.9): 100%|██████████| 3/3 [00:06<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 61.9 with parameters ['Predictor 1: Instruction 5', 'Predictor 1: Few-Shot Set 4'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9]\n",
      "Best score so far: 64.1\n",
      "========================\n",
      "\n",
      "\n",
      "===== Trial 10 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:06<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 14', 'Predictor 1: Few-Shot Set 1'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 11 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 3', 'Predictor 1: Few-Shot Set 1'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 12 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.857 / 3  (61.9): 100%|██████████| 3/3 [00:06<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 61.9 with parameters ['Predictor 1: Instruction 0', 'Predictor 1: Few-Shot Set 4'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 13 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.857 / 3  (61.9): 100%|██████████| 3/3 [00:00<00:00, 52.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 61.9 with parameters ['Predictor 1: Instruction 5', 'Predictor 1: Few-Shot Set 4'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 14 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.9229940764674205 / 3  (64.1): 100%|██████████| 3/3 [00:00<00:00, 48.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 64.1 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 1'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 15 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:10<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 10'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 16 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.774193548387097 / 3  (59.1): 100%|██████████| 3/3 [00:05<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.14 with parameters ['Predictor 1: Instruction 13', 'Predictor 1: Few-Shot Set 3'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 17 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:14<00:00,  4.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 16', 'Predictor 1: Few-Shot Set 6'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 18 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.9229940764674205 / 3  (64.1): 100%|██████████| 3/3 [00:00<00:00, 49.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 64.1 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 1'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9, 64.1]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 19 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.8571428571428572 / 3  (61.9): 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 61.9 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 1'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9, 64.1, 61.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 20 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:06<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 9', 'Predictor 1: Few-Shot Set 1'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9, 64.1, 61.9, 59.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 21 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:06<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 17', 'Predictor 1: Few-Shot Set 8'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9, 64.1, 61.9, 59.9, 59.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 22 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.9229940764674205 / 3  (64.1): 100%|██████████| 3/3 [00:00<00:00, 28.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 64.1 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 1'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9, 64.1, 61.9, 59.9, 59.9, 64.1]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 23 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.6 / 3  (53.3): 100%|██████████| 3/3 [00:05<00:00,  1.70s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 53.33 with parameters ['Predictor 1: Instruction 6', 'Predictor 1: Few-Shot Set 17'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9, 64.1, 61.9, 59.9, 59.9, 64.1, 53.33]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 24 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.7077464788732395 / 3  (56.9): 100%|██████████| 3/3 [00:07<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 56.92 with parameters ['Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 0'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9, 64.1, 61.9, 59.9, 59.9, 64.1, 53.33, 56.92]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "===== Trial 25 / 25 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.796875 / 3  (59.9): 100%|██████████| 3/3 [00:06<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 59.9 with parameters ['Predictor 1: Instruction 18', 'Predictor 1: Few-Shot Set 11'].\n",
      "Scores so far: [56.92, 59.9, 59.14, 50.0, 56.92, 59.9, 64.1, 59.9, 59.9, 61.9, 59.9, 59.9, 61.9, 61.9, 64.1, 59.9, 59.14, 59.9, 64.1, 61.9, 59.9, 59.9, 64.1, 53.33, 56.92, 59.9]\n",
      "Best score so far: 64.1\n",
      "=========================\n",
      "\n",
      "\n",
      "Returning best identified program with score 64.1!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tp = dspy.MIPROv2(\n",
    "    metric=metric, auto=\"medium\", num_threads=24\n",
    ")  # use fewer threads if your rate limit is small\n",
    "\n",
    "optimized_rag = tp.compile(\n",
    "    RAG(),\n",
    "    trainset=data[:7],\n",
    "    valset=data[7:],\n",
    "    max_bootstrapped_demos=2,\n",
    "    max_labeled_demos=2,\n",
    "    requires_permission_to_run=False,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.944210822284272 / 10  (79.4): 100%|██████████| 10/10 [00:06<00:00,  1.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_41a20 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_41a20 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_41a20_row0_col0, #T_41a20_row0_col1, #T_41a20_row0_col2, #T_41a20_row0_col3, #T_41a20_row0_col4, #T_41a20_row1_col0, #T_41a20_row1_col1, #T_41a20_row1_col2, #T_41a20_row1_col3, #T_41a20_row1_col4, #T_41a20_row2_col0, #T_41a20_row2_col1, #T_41a20_row2_col2, #T_41a20_row2_col3, #T_41a20_row2_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_41a20\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_41a20_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_41a20_level0_col1\" class=\"col_heading level0 col1\" >example_response</th>\n",
       "      <th id=\"T_41a20_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n",
       "      <th id=\"T_41a20_level0_col3\" class=\"col_heading level0 col3\" >pred_response</th>\n",
       "      <th id=\"T_41a20_level0_col4\" class=\"col_heading level0 col4\" >SemanticF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_41a20_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_41a20_row0_col0\" class=\"data row0 col0\" >What is the primary innovation introduced by the Transformer model?</td>\n",
       "      <td id=\"T_41a20_row0_col1\" class=\"data row0 col1\" >The Transformer model introduces an architecture based solely on self-attention, eliminating recurrent and convolutional layers.</td>\n",
       "      <td id=\"T_41a20_row0_col2\" class=\"data row0 col2\" >['Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU...</td>\n",
       "      <td id=\"T_41a20_row0_col3\" class=\"data row0 col3\" >The primary innovation introduced by the Transformer model is the self-attention mechanism, which enables the model to weigh the importance of different words in a...</td>\n",
       "      <td id=\"T_41a20_row0_col4\" class=\"data row0 col4\" >✔️ [0.708]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41a20_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_41a20_row1_col0\" class=\"data row1 col0\" >Why does the Transformer allow for faster training than RNN-based models?</td>\n",
       "      <td id=\"T_41a20_row1_col1\" class=\"data row1 col1\" >The Transformer enables parallelization across input sequences, reducing training time significantly compared to RNNs.</td>\n",
       "      <td id=\"T_41a20_row1_col2\" class=\"data row1 col2\" >['Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder,...</td>\n",
       "      <td id=\"T_41a20_row1_col3\" class=\"data row1 col3\" >The Transformer allows for faster training than RNN-based models because it relies entirely on attention mechanisms, enabling parallel processing of input sequences. This contrasts with...</td>\n",
       "      <td id=\"T_41a20_row1_col4\" class=\"data row1 col4\" >✔️ [0.857]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41a20_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_41a20_row2_col0\" class=\"data row2 col0\" >How does self-attention work in the Transformer model?</td>\n",
       "      <td id=\"T_41a20_row2_col1\" class=\"data row2 col1\" >Self-attention allows each position in a sequence to attend to other positions, capturing dependencies regardless of their distance.</td>\n",
       "      <td id=\"T_41a20_row2_col2\" class=\"data row2 col2\" >['Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder,...</td>\n",
       "      <td id=\"T_41a20_row2_col3\" class=\"data row2 col3\" >Self-attention in the Transformer model works by allowing each position in the input sequence to attend to all other positions. It uses the same input...</td>\n",
       "      <td id=\"T_41a20_row2_col4\" class=\"data row2 col4\" >✔️ [0.933]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14798fc80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 7 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "79.44"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(optimized_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Prediction(\n",
       "     question_type='text'\n",
       " ),\n",
       " Prediction(\n",
       "     question_type='video'\n",
       " ))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QuestionType(dspy.Signature):\n",
    "    \"\"\"Classify whether it is a question about text, video, audio.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    question_type = dspy.OutputField()\n",
    "\n",
    "\n",
    "classify = dspy.Predict(QuestionType)\n",
    "classify(question=\"Give me a summary about chapter 2\"), classify(\n",
    "    question=\"What is shown at the 3 minute mark?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
